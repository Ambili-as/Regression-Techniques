{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "075879f8",
   "metadata": {},
   "source": [
    "### Regression Assignment\n",
    "#### Objective:\n",
    " ##### The objective of this assignment is to evaluate your understanding of regression techniques in supervised learning by applying them to a real-world dataset.\n",
    "##### Dataset: Use the California Housing dataset available in the sklearn library. This dataset contains information about various features of houses in California and their respective median prices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3326381d",
   "metadata": {},
   "source": [
    "#### 1.Loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "478d93a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will load the California Housing dataset using the fetch_california_housing() function from sklearn.datasets.\n",
    "#After that, we will convert the dataset into a pandas DataFrame for easier manipulation.\n",
    "\n",
    "#Lets import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#loading the dataset and converting into pd dataframe\n",
    "data = fetch_california_housing()\n",
    "X = pd.DataFrame(data.data,columns=data.feature_names)\n",
    "X\n",
    "y = pd.Series(data.target, name='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9936235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MedInc        0\n",
       "HouseAge      0\n",
       "AveRooms      0\n",
       "AveBedrms     0\n",
       "Population    0\n",
       "AveOccup      0\n",
       "Latitude      0\n",
       "Longitude     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "missing = X.isnull().sum()\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No missing values in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdf04f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.34476576,  0.98214266,  0.62855945, ..., -0.04959654,\n",
       "         1.05254828, -1.32783522],\n",
       "       [ 2.33223796, -0.60701891,  0.32704136, ..., -0.09251223,\n",
       "         1.04318455, -1.32284391],\n",
       "       [ 1.7826994 ,  1.85618152,  1.15562047, ..., -0.02584253,\n",
       "         1.03850269, -1.33282653],\n",
       "       ...,\n",
       "       [-1.14259331, -0.92485123, -0.09031802, ..., -0.0717345 ,\n",
       "         1.77823747, -0.8237132 ],\n",
       "       [-1.05458292, -0.84539315, -0.04021111, ..., -0.09122515,\n",
       "         1.77823747, -0.87362627],\n",
       "       [-0.78012947, -1.00430931, -0.07044252, ..., -0.04368215,\n",
       "         1.75014627, -0.83369581]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets standardize the features using StandardScalar\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1a42a",
   "metadata": {},
   "source": [
    "#### Explanation of Preprocessing:\n",
    "#### Missing Values: If there are missing values, it’s important to handle them to ensure that the model is not biased or inefficient due to incomplete data. In this dataset no missing values are there.\n",
    "#### Feature Scaling: Standardizing the features ensures that all features are on a similar scale, preventing models from being biased toward features with larger scales. Standardization helps gradient-based algorithms converge faster and improves model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8799865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets split the data into training and testing sets.\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_scaled,y,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b75ec",
   "metadata": {},
   "source": [
    "### 2.Regression Algorithm Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aaed85",
   "metadata": {},
   "source": [
    "#### i)Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "64ca0168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.00412998, 2.57561917, 1.19769801, ..., 2.42460902, 1.73085934,\n",
       "       1.35765516])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#Initializing and fit the linear regression model\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train,y_train)\n",
    "#MAking predictions\n",
    "linear_pred = linear_reg.predict(X_test)\n",
    "linear_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31438991",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "##### Linear Regression assumes a linear relationship between the features and the target variable.  If the relationship between features and target is roughly linear, this model will perform well.\n",
    "##### The California Housing dataset includes features like income, house age, and average rooms, which might have linear relationships with the target variable, so starting with linear regression is a good choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed8adc",
   "metadata": {},
   "source": [
    "#### ii)Decision Tree Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8c9b302",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.607, 2.294, 0.768, ..., 1.986, 2.264, 0.827])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary models\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "#Initializing Decision tree regression\n",
    "dt_reg = DecisionTreeRegressor()\n",
    "dt_reg.fit(X_train,y_train)\n",
    "#Making predictions\n",
    "dt_pred = dt_reg.predict(X_test)\n",
    "dt_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a2e5f",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "##### A Decision Tree Regressor creates a tree-like structure by splitting the data at each node based on the best feature that reduces the variance within the split. This process continues until the data is split into smaller, homogenous groups (leaf nodes). Each leaf node provides a prediction by averaging the target values of the instances in that node.\n",
    "##### Since the California Housing dataset may contain non-linear relationships between features and the target, Decision Trees can model such interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c1a9ca",
   "metadata": {},
   "source": [
    "#### iii)Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6140d5a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.95706, 2.37913, 0.74204, ..., 1.90402, 1.96961, 0.99142])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#Initializing Random forest regression\n",
    "rf_reg = RandomForestRegressor()\n",
    "rf_reg.fit(X_train,y_train)\n",
    "#Making predictions\n",
    "rf_pred = rf_reg.predict(X_test)\n",
    "rf_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d657395",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "##### Random Forest is an ensemble learning method that creates multiple decision trees and aggregates their predictions. The final prediction is the average of all the trees' predictions, which helps to reduce overfitting and improve generalization.\n",
    "##### Random Forest can model complex non-linear relationships It works well on high-dimensional datasets, like the California Housing dataset, where many features may interact with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5980cc2b",
   "metadata": {},
   "source": [
    "#### iv)Gradient boosting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "609a7539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.81656876, 2.30660447, 0.8132348 , ..., 2.11724067, 2.03201091,\n",
       "       0.91179624])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary models\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "#Initializing Gradient Boosting regression\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "gb_reg.fit(X_train,y_train)\n",
    "#Making predictions\n",
    "gb_pred = gb_reg.predict(X_test)\n",
    "gb_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1739942",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "##### Gradient Boosting builds models sequentially. Each new model attempts to correct the residual errors of the previous model\n",
    "##### Gradient Boosting is highly effective for datasets with complex, non-linear relationships. In the case of California Housing, it can model the non-linear effects of features on house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9048110",
   "metadata": {},
   "source": [
    "#### v)Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c5b2948f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.72930462, 2.2756762 , 0.76080943, ..., 2.05595118, 1.76671968,\n",
       "       0.98554852])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing necessary models\n",
    "from sklearn.svm import SVR\n",
    "#Initializing Gradient Boosting regression\n",
    "svr_reg = SVR()\n",
    "svr_reg.fit(X_train,y_train)\n",
    "#Making predictions\n",
    "svr_pred = svr_reg.predict(X_test)\n",
    "svr_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38806227",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "##### SVR can model non-linear relationships using kernel functions, which can be helpful for datasets with complex feature-target relationships. It works well when the dataset has fewer outliers, as it is sensitive to them.\n",
    "##### The California Housing dataset, with multiple features influencing house prices, might benefit from SVR's ability to handle non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a392b4e1",
   "metadata": {},
   "source": [
    "### 3. Model Evaluation and Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01a6cf",
   "metadata": {},
   "source": [
    "##### Lets Evaluate the performance of each algorithm using the following metrics:\n",
    "##### Mean Squared Error (MSE) , Mean Absolute Error (MAE) , R-squared Score (R²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f39b9abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets import necessary libraries\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b322cc",
   "metadata": {},
   "source": [
    "#### i)Linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "750422e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predication made is stored in linear_pred\n",
    "#Lets calculate evaluation metrices\n",
    "#Mean squared error metric\n",
    "mse_linear = mean_squared_error(y_test,linear_pred)\n",
    "#Mean absolute error\n",
    "mae_linear = mean_absolute_error(y_test,linear_pred)\n",
    "#r2score\n",
    "r2_linear = r2_score(y_test,linear_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46b151c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression Evaluation: \n",
      "Mean Squared Error:  0.5417517275769405\n",
      "Mean absolute Error:  0.538957248055476\n",
      "R2 score:  0.6075794091011186\n"
     ]
    }
   ],
   "source": [
    "#Lets print the results\n",
    "print(\"Linear regression Evaluation: \")\n",
    "print(\"Mean Squared Error: \",mse_linear)\n",
    "print(\"Mean absolute Error: \",mae_linear)\n",
    "print(\"R2 score: \",r2_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad121e3",
   "metadata": {},
   "source": [
    "#### ii) Decision tree regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c00fc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predication made is stored in dt_pred\n",
    "#Lets calculate evaluation metrices\n",
    "#Mean squared error metric\n",
    "mse_dt = mean_squared_error(y_test,dt_pred)\n",
    "#Mean absolute error\n",
    "mae_dt = mean_absolute_error(y_test,dt_pred)\n",
    "#r2score\n",
    "r2_dt = r2_score(y_test,dt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "833371b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Regression Evaluation: \n",
      "Mean Squared Error:  0.5256955741617491\n",
      "Mean absolute Error:  0.4665009617248062\n",
      "R2 score:  0.619209764649653\n"
     ]
    }
   ],
   "source": [
    "#Lets print the results\n",
    "print(\"Decision Tree Regression Evaluation: \")\n",
    "print(\"Mean Squared Error: \",mse_dt)\n",
    "print(\"Mean absolute Error: \",mae_dt)\n",
    "print(\"R2 score: \",r2_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef4c4a7",
   "metadata": {},
   "source": [
    "#### iii)Random Forest Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31e97c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predication made is stored in rf_pred\n",
    "#Lets calculate evaluation metrices\n",
    "#Mean squared error metric\n",
    "mse_rf = mean_squared_error(y_test,rf_pred)\n",
    "#Mean absolute error\n",
    "mae_rf = mean_absolute_error(y_test,rf_pred)\n",
    "#r2score\n",
    "r2_rf = r2_score(y_test,rf_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f39d2d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression Evaluation: \n",
      "Mean Squared Error:  0.2736603968684209\n",
      "Mean absolute Error:  0.339245191521318\n",
      "R2 score:  0.8017727139975268\n"
     ]
    }
   ],
   "source": [
    "#Lets print the results\n",
    "print(\"Random Forest Regression Evaluation: \")\n",
    "print(\"Mean Squared Error: \",mse_rf)\n",
    "print(\"Mean absolute Error: \",mae_rf)\n",
    "print(\"R2 score: \",r2_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b831d5",
   "metadata": {},
   "source": [
    "#### iv)Gradient Boosting Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d034b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predication made is stored in gb_pred\n",
    "#Lets calculate evaluation metrices\n",
    "#Mean squared error metric\n",
    "mse_gb = mean_squared_error(y_test,gb_pred)\n",
    "#Mean absolute error\n",
    "mae_gb = mean_absolute_error(y_test,gb_pred)\n",
    "#r2score\n",
    "r2_gb = r2_score(y_test,gb_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38f3454e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Regression Evaluation: \n",
      "Mean Squared Error:  0.29882813115581697\n",
      "Mean absolute Error:  0.37517686471055917\n",
      "R2 score:  0.7835423389790304\n"
     ]
    }
   ],
   "source": [
    "#Lets print the results\n",
    "print(\"Gradient Boosting Regression Evaluation: \")\n",
    "print(\"Mean Squared Error: \",mse_gb)\n",
    "print(\"Mean absolute Error: \",mae_gb)\n",
    "print(\"R2 score: \",r2_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458e41c",
   "metadata": {},
   "source": [
    "#### v)Support Vector Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d27cb960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predication made is stored in svr_pred\n",
    "#Lets calculate evaluation metrices\n",
    "#Mean squared error metric\n",
    "mse_svr = mean_squared_error(y_test,svr_pred)\n",
    "#Mean absolute error\n",
    "mae_svr = mean_absolute_error(y_test,svr_pred)\n",
    "#r2score\n",
    "r2_svr = r2_score(y_test,svr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7c0c8de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Regression Evaluation: \n",
      "Mean Squared Error:  0.36526277205789964\n",
      "Mean absolute Error:  0.3995027521112096\n",
      "R2 score:  0.7354200724279787\n"
     ]
    }
   ],
   "source": [
    "#Lets print the results\n",
    "print(\"Support Vector Regression Evaluation: \")\n",
    "print(\"Mean Squared Error: \",mse_svr)\n",
    "print(\"Mean absolute Error: \",mae_svr)\n",
    "print(\"R2 score: \",r2_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd96421",
   "metadata": {},
   "source": [
    "##### To compare the results of all the regression models (Linear Regression, Decision Tree Regressor, Random Forest Regressor, Gradient Boosting Regressor, and Support Vector Regressor), we will look at the three evaluation metrics for each model: Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared (R²)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b4aecbca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression Evaluation: \n",
      "Mean Squared Error:  0.5417517275769405\n",
      "Mean absolute Error:  0.538957248055476\n",
      "R2 score:  0.6075794091011186\n",
      "\n",
      "Decision Tree Regression Evaluation: \n",
      "Mean Squared Error:  0.5256955741617491\n",
      "Mean absolute Error:  0.4665009617248062\n",
      "R2 score:  0.619209764649653\n",
      "\n",
      "Random Forest Regression Evaluation: \n",
      "Mean Squared Error:  0.2736603968684209\n",
      "Mean absolute Error:  0.339245191521318\n",
      "R2 score:  0.8017727139975268\n",
      "\n",
      "Gradient Boosting Regression Evaluation: \n",
      "Mean Squared Error:  0.29882813115581697\n",
      "Mean absolute Error:  0.37517686471055917\n",
      "R2 score:  0.7835423389790304\n",
      "\n",
      "Support Vector Regression Evaluation: \n",
      "Mean Squared Error:  0.36526277205789964\n",
      "Mean absolute Error:  0.3995027521112096\n",
      "R2 score:  0.7354200724279787\n"
     ]
    }
   ],
   "source": [
    "#Lets compare the evaluation results:\n",
    "#Lets print the results\n",
    "print(\"Linear regression Evaluation: \")\n",
    "print(\"Mean Squared Error: \",mse_linear)\n",
    "print(\"Mean absolute Error: \",mae_linear)\n",
    "print(\"R2 score: \",r2_linear)\n",
    "#Lets print the results\n",
    "print(\"\\nDecision Tree Regression Evaluation: \")\n",
    "print(\"Mean Squared Error: \",mse_dt)\n",
    "print(\"Mean absolute Error: \",mae_dt)\n",
    "print(\"R2 score: \",r2_dt)\n",
    "#Lets print the results\n",
    "print(\"\\nRandom Forest Regression Evaluation: \")\n",
    "print(\"Mean Squared Error: \",mse_rf)\n",
    "print(\"Mean absolute Error: \",mae_rf)\n",
    "print(\"R2 score: \",r2_rf)\n",
    "#Lets print the results\n",
    "print(\"\\nGradient Boosting Regression Evaluation: \")\n",
    "print(\"Mean Squared Error: \",mse_gb)\n",
    "print(\"Mean absolute Error: \",mae_gb)\n",
    "print(\"R2 score: \",r2_gb)\n",
    "#Lets print the results\n",
    "print(\"\\nSupport Vector Regression Evaluation: \")\n",
    "print(\"Mean Squared Error: \",mse_svr)\n",
    "print(\"Mean absolute Error: \",mae_svr)\n",
    "print(\"R2 score: \",r2_svr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75251def",
   "metadata": {},
   "source": [
    "##### Best-Performing Algorithm:\n",
    "##### Lower MSE and MAE: These indicate the model's prediction is closer to the actual values, which generally translates to better performance. Higher R²: A higher R² value indicates that the model is better at explaining the variance in the target variable (i.e., it fits the data well)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8e4b85",
   "metadata": {},
   "source": [
    "##### \" Here in this Project, We got Lower MSE and MAE , Higher R^2 value for Random forest Regression model, So we can conclude that Random forest model's the Best performing Algorithm. Also Gradient boosting Regression model evaluated as a good performing algorithm \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905df48",
   "metadata": {},
   "source": [
    "##### Worst-Performing Algorithm:\n",
    "##### Higher MSE and MAE: Models with higher values of MSE and MAE are not predicting well. Lower R²: A lower R² means the model doesn't explain much of the variance in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ace7d",
   "metadata": {},
   "source": [
    "##### \" Here in this Project, We got Lower MSE and MAE , Higher R^2 value for Linear Regression mode and Decision tree model, So we can conclude that Linear regression Algorithm and DEcision tree Algorithm are both performing worst in this case. \""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
